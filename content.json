{"meta":{"title":"Hexo","subtitle":"","description":"","author":"keefe","url":"http://blog.357573.com","root":"/"},"pages":[{"title":"tags","date":"2020-01-02T09:04:19.000Z","updated":"2020-01-02T09:11:55.630Z","comments":true,"path":"tags/index.html","permalink":"http://blog.357573.com/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"目标检测-YOLOv2","slug":"目标检测-YOLOv2","date":"2020-01-05T14:01:45.000Z","updated":"2020-01-06T00:46:41.565Z","comments":true,"path":"2020/01/05/目标检测-YOLOv2/","link":"","permalink":"http://blog.357573.com/2020/01/05/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B-YOLOv2/","excerpt":"","text":"yolov2从网络上讲是很简单的结构,几乎和RPN网络完全一样,但是它集百家之长,增加了很多细节去优化. 先说一下前传流程,他的网络是作者自创的darknet,全部由卷积层和最大池化层构成,只有池化层是缩放了的,每个池化层步长都是2,一共5个池化层,所以最后会缩小2的5次方,也就是32倍.最后输出是什么呢?这个图上画的是softmax,其实不是的啊,这个softmax是他做pretrain的时候用的,后面就舍弃了,这个我们之后会介绍的.所以它的输出实际上和yolov1是一样的,是一个特征向量,例如预测具有80种分类的coco80上的输出就是13*13*425维的特征,只不过它是卷积出来的,完全没有经历全连接层.13*13代表的是最后那层featuremap是13*13的维度,而每个点预测5种bbox,每个bbox是80个类别的置信度+4个坐标(x,y,w,h)四个数,再加一个是物体的概率.这里和YOLOv1一样的,把是不是物体这个概率单独拿了出来,其实反过来想就相当于是背景的概率了.所以80个类别那每个bbox就预测85个值,每个点5个bbox就是5*85=425了.因为没有了全连接层了,所以其实YOLOv2理论上是可以支持任意大小的图的,也就是说最后那层featuremap不一定限制成立13*13,具体大小也就是你图宽高分别/32以后的值. 然后再说说它具体优点,他的优点分为3个方向,效果,速度和鲁棒性,我们先说效果方面,第一个是添加了batchnorm,提高了大概2%的map,并且有了batchnorm,可以移除dropout并且不用担心过拟合了. 第二个是高分辨率,YOLO V1在训练分类的时候用的是224的分辨率,训练检测的时候把他扩充到了448,而现在用448的又finetune了一下分类网络,map又提升了4%. 第三个是加了anchor,把YOLO里的全连接层删掉了,然后把网络输入又修改成了416的,目的是卷积到最后能够得到一个奇数,以保证每个最后输出的featuremap有一个中心.网络总共缩放了32倍,所以416/32是13,刚好有个中心点,其实真正训练的时候并没有严格遵循奇数原则,实际测试的时候更是支持任意尺度的,比方说它最新的代码里输入就是608的,那么最后输出的就是19.yolov1最终输出的是7*7的featuremap,每个点输出两个bbox,所以一共也就98个可能,而用了anchor之后每个点有5种不同的bbox了,那么总共就有了13*13*5=845个可能了,这样虽然准确率下降了一点点,从69.5降到了69.2,但是召回率大大的提高了,从81%提高到了88%. 第四个是这些anchor不再是纯自动学习出来的了,而是人为制定的了.虽然训练出来的网络最后也会微调这个框的位置,但是如果我们在初始化的时候就让这个框更好一点那么结果也会更好一点,所以我们就从训练图片中聚类出来这5个尺度.但是聚类有一个问题,就是假设用欧氏距离的kmeans的话,那么尺寸越大的框他的误差也就越大,所以就用IOU来替代欧氏距离的误差了. 第五个是anchor位置预测的技巧,RPN网络是直接回归出相对于全图的中心点x和y,这样的话可能会乱跳,比方说物体在图中心,那么所有的x和y都指向那里了,很不利于收敛.现在它做的改进就是把deltax和deltay都加了一层sigmoid,这样把他们限制在0-1之间了,这样就出不了一个当前的网格了,这样网络更加稳定了,提升了几乎5个百分点. 第六个是精细化的特征提取,这里其实是说最后那层featuremap太小了,可能提取的不够精细,像SSD都知道从之前层里的featuremap提取边框信息,那么我们也这样做试试,所以就把最后一个池化之前的那个featuremap提取出来,衔接到之后一层,就像resnet那样连接过去,但是尺度不一样,所以做了一次reshape,简单说由于这层featuremap是下一层的两倍大小,所以每4个格子对应下面一层的一个格子,那就干脆把这4个变成通道数,这样出来的featuremap就和后面的大小一样了.这一层叫做paththrough,比方说本来是26*26*512的featuremap,把他reshape到13*13*2048,然后就可以和后面那一层13*13*1024的输出合并了,变成13*13*3072的输出了.这一步也提高了1%的准确率. 第七个是YOLOv2最大的创新,他用了多种尺度的图去训练,其实这点yolov1也干过,不过v2更方便了,因为它没有了全连接层,输入想怎么变就怎么变了.只要保证是32的倍数就行了,所以从最小最大608的公倍数都试了一遍,这里很搞笑,他也不再关注输出是不是奇数了,所以那句建议真正落实起来还是挺困难的. 接着再说速度方面的改进,网络基本沿袭了VGG的架构,但是比VGG要略小,所以速度更快.VGG所有卷积全是3*3的,而darknet加入了1*1的过度层,有人说他减少了参数,实际上并没有,因为它的过渡层维度有原先维度的一半,假设原先是512维变成512维,那么计算有512*512这么多倍,而中间加上一个256的1*1卷积,这样总共计算就有512*256+512*256,最后结果竟然是一样的,所以他的作用并不是减少了参数,而是减小了计算量,因为原来计算量是3*3*w*h*512*512,而现在卷积核是1*1的了,也就是1*1*w*h*512*256+3*3*w*h*256*512,计算量还是小了一半的,而且因为多了一层而增加了非线性.另外还有batchnorm也能够加速收敛. 网络先在imagenet上训练了160次分类器,其实主要就是学习提取特征的能力.并且这个准确率还是相当可以的,1000个分类准确率就能达到76.5 然后训练检测器,删掉最后一层卷积层,又添加3个3*3的卷积层,就变成之前看到的darknet的样子. 最后一个优点是健壮性,刚才我们看到他在训练的时候是把检测和分类分开的,这就是他健壮性所在,如果只有检测数据它就只回归检测的参数,如果只有分类数据就只回归分类的参数.双方互不干扰. 我们来看看结果,用288的图做输入,效果几乎赶上了fastrcnn,而速度达到了90帧.其实论文到这里还没有结束,论文提出了一个非常好的问题,就是子类的问题,比方说哈士奇,田园犬都属于狗,所以在大类别的时候把他们分得太开反而会造成网络不容易收敛,后面介绍了怎么去区分子类,解决了这个问题,这个网络就可以区分9000种类别了.之后在介绍YOLO9000的时候我们会详细的介绍这一点.好,YOLOv2我们就介绍到这里了,谢谢大家.","categories":[{"name":"论文解读","slug":"论文解读","permalink":"http://blog.357573.com/categories/%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB/"}],"tags":[{"name":"目标检测","slug":"目标检测","permalink":"http://blog.357573.com/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"},{"name":"论文解读","slug":"论文解读","permalink":"http://blog.357573.com/tags/%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB/"},{"name":"算法","slug":"算法","permalink":"http://blog.357573.com/tags/%E7%AE%97%E6%B3%95/"},{"name":"图像处理","slug":"图像处理","permalink":"http://blog.357573.com/tags/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/"},{"name":"YOLO","slug":"YOLO","permalink":"http://blog.357573.com/tags/YOLO/"}]},{"title":"目标检测-YOLO","slug":"目标检测-YOLO","date":"2020-01-05T13:54:38.000Z","updated":"2020-01-06T00:46:41.557Z","comments":true,"path":"2020/01/05/目标检测-YOLO/","link":"","permalink":"http://blog.357573.com/2020/01/05/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B-YOLO/","excerpt":"","text":"RCNN系列出来后对整个目标检测起到了颠覆性的作用，但是由于过于复杂的流程，使得代码实现以及运行速度都很慢。此时，一位二次元大叔站了出来，提出了我们要舍弃这种繁琐， yolo！yolo问世的时代是在fasterrcnn之后，所以论文主要是和rcnn系列做对比，其实这也应证了rcnn系列在目标检测领域的的地位，出的新算法都来和他们对比。那么yolo和fastrcnn比有什么优势呢。 第一个是快速，yolo没有region proposal那一步，在Titanx上可以达到45帧。 第二个是全局化,yolo是整张图输入的，对于全局信息，尤其是背景的判断要比fastrcnn好。因为fastrcnn都是拆开一块一块的输入，失去了重要的全局信息。 第三个是泛化,YOLO可以学到更泛化的特征,即在真实照片上训练,但是在画上面识别,他的这个泛化效果是目前世界最好的. 当然,文章也说了YOLO存在的一些缺点,第一个就是精度不是最好的,毕竟YOLO是在速度上的改革,可以说是一场速度革命了,抛弃了RCNN系列繁琐的流程,找到了一个速度与精度的平衡点. 其次对于定位的精度也不太好,框很有可能预测的比较偏 最后还有对小件很不友好,这是这个算法的原理导致的,因为他最后做了一步类似于压缩的步骤. 我们来看一下YOLO的具体流程,YOLO文章说他是把图像切成S*S个小方格来检测的,其实这样说容易让人产生误会,误认为它还是和RCNN那样先选出一些region proposal,只不过这次是直接切割出来的了,然后再分别输入神经网络预测.其实YOLO并不是这样的,实际上YOLO就是一个很普通的卷积神经网络,最后靠着全连接层输出了30个S*S长度的特征. 我们来观察一下这个结构图,首先输入的是448*448大小的图,经过了一开始的步长为2的卷积,变成了224*224的featuremap,然后再经过步长为2的池化,就变成了112*112的featuremap了,也就是图上的第一个输出,接着经过了一系列步长为2的操作,最终缩放成了7*7的featuremap了,此时用全连接层去处理它,输出了一个4096的全连接特征,然后再加上一层全连接,输出d就是7*7*30的特征,这个图上画的是个7*7*30的featuremap,实际上这个也是在误导人,根据我仔细观察各个版本的代码实现,最后那一层就是个全连接层,输出的其实是个向量,那个向量的长度是7*7*30=1470,为了形象的表达所以在图上又给他画成了三维的形状.所以大家不要被误导了,最后一层是通过全连接层输出的1470维的特征.那你要问了,我们这是个检测网络呀,要输出特征干嘛呢,最后是不是像fastrcnn那样后面再跟一个回归层呢,用这个特征q乘以坐标的参数得到具体位置呢?其实如果这样那这个就成了fastrcnn了,而yolo是完全摒弃了rcnn的繁琐步骤的,所以才叫you only look once,我们得到的这1470维的特征已经包含了所有物体的位置以及概率了.这是怎么实现的呢,我们就要来看看他为什么非要误导我们把最后那个特征画成7*7*30这种形状.因为抽象来讲,yolo是把图像切分成S*S块,然后来分别判断每一块是不是包含了要检测的对象,他的概率是多少,具体坐标偏移量是多少,而这个S预设的就是7. 所以到最后的那个特征,我们可以想象成是把图像切分成了7*7个小格子得来的,其实看之前那个层,在全连接之前,确实是缩放到了7*7了,而且步长一直都是2,所以其实这7*7个点确实是从这些格子里提取出来的特征. 再说为什么是30,我们希望每个格子给出一个两个坐标和置信度,就是说以这个格子为中心的物体,给出两组猜测结果,坐标自然是xywh四个了,置信度就是这个框是个物体的概率,所以坐标加上置信度一共是5位了,我们希望每个格子预测出两个bbox出来,那么就是10位了.然后还要输出每个格子代表每种物体的概率是多少,因为Pascal的数据是20类,那么就再加上20个位置放概率,所以总共就是30位了. 这里容易搞混淆,明明不是每个bbox都输出了一个置信度吗,怎么还又输出一个概率呢?而且为什么置信度的数量是*2了的,概率为什么不用乘呢? 其实我们说每一个格子都会预测出2个bbox,选出两个来就是为了增加一种可能,在现在最新版的YOLO代码里他就改成了3个,所以最后输出的是7*7*35长度的向量了.当然这个数字是可以任意变动的,只要能找到效率的平衡点就好.因为YOLO已经把图缩放成了7*7了,所以现在这个精度很低了,所以我们多提供一个候选框,其实就是在减小风险,避免本身这个物体就落在你这一格里,结果你却不负责任的预测错了.你这一格管着这么大的面积,万一我这个东西就只存在你这一格里面,你预测错了岂不就是没有补救的机会了?所以我们就让一格提供2个bbox,然后呢每个bbox的位置是不一样的,但是中心肯定是在这个格子里的,具体形状不一样,就像这张图上一样,同样中心在这个格子里,但是有的是长条有的是短的,有的大有的小一样.我们每个格子选出2个候选框,就有2组坐标,2个置信度,这个置信度只表示我这个候选框里有要找的物体的概率,不代表是哪种物体,我们可以简单理解成最后从这2个候选框中选择置信度最高的那个作为最终这一格子的候选框,而之后那20个物体的概率,指的就是我们这个格子代表的是每种物体的概率了.那你要问凭什么你规定他是什么他就是什么呢?这里就是神经网络神奇的地方了,想要什么就有什么,完全是通过训练来规定出来的,我们一直规定前面20位表示每种物体的概率,训练的时候也一直用概率来评判他,自然他就会乖乖的变成概率了,后面的是坐标,训练时候也一直用坐标来约束他,他也慢慢的就x了,认识到这里是坐标了. 我们来看他的损失函数,损失函数是把我们刚才介绍的那几项分别求出损失值然后加起来的,使用了最简单的平方差损失函数,预测坐标-实际坐标平方求和,预测概率-实际概率平方求和.其中我们可以看到这里有2个lambda,这是因为大部分的格子都是没有物体的,而没有物体真实的概率就是0,这样很容易就把网络带偏了,所以这里加上一个lambda来减小它的影响,这里的noobj就是没有物体的意思,没有物体时候的置信度误差*了一个lambda,这个lambda设置的是0.5,而有物体时候的lambda设置的是5,这样就可以平衡两者的影响了.然后x和y代表的是中心点坐标,这个中心店肯定是存在单元格内的,她的值是在单元格内的比例,所以是在0-1之间的,比方说这个点正好在这一格的正中间的话那么x和y就都是0.5,w和h肯定不会局限在单元格内,因为这个格子只代表物体的中心,物体整个d大小是可以大到整张图的,所以w和h就用的是和整张图的比例,这样也把他限制在了0-1之间了,由于大的目标这个误差就很大,所以再开个根号来减小物体过大的影响. 由于每个格子只会输出一个物体,那么如果训练的时候有两个物体落在这个格子里该怎么办呢?YOLO的做法是根据IOU找到IOU最大的一个物体训练,也就是先预测一波,然后看哪个bbox和实际标注的交并比最大就选择那个. yolo在训练的时候其实是先用imagenet做了预训练的,也就是首先训练一个分类器出来,使得网络具有提取特征的能力了,然后再去按照yolo的规则训练检测器,他的这个检测器实际上就是一种回归的方法,因为全程都是用的cnn算出来的值,激活函数用的是prelu,首先relu是最快的激活函数了,因为他几乎就没有激活什么,只是去掉了负值,而prelu让负的也有了意义,yolo设置的p是0.1,也就是说负值的话就是输出0.1x,正值就直接输出x.训练的时候学习率最初设为10-3,然后慢慢调到10-2,为什么是从低向高呢,因为这只是开始阶段,如果一开始调的是10-2他会出现梯度乱跳的问题,毕竟是从预训练模型过来迁移学习的嘛,紧接着到了10-2之后跑75次,然后再降到10-3,跑30此之后降到10-4再跑30次结束. 再来看结果,YOLO的准确率差不多是百分之六十多,fasterrcnn是70,但是fasterrcnn只能跑到7帧,而yolo却可以跑到实时的级别,速度提高了五倍有余,并且YOLO还有一个快速版的,就是把网络阉割了一些层,效果差了一点,但是速度能达到150帧了,已经数据超级快速的网络了. YOLOv1毕竟也是一个早期的作品了,他的革命性在于颠覆了rcnn系列繁琐的步骤,使得整个检测用一个神经网络做回归就得出来了,为后来的ssd等网络的兴起提供了强大的自信心保证.","categories":[{"name":"论文解读","slug":"论文解读","permalink":"http://blog.357573.com/categories/%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB/"}],"tags":[{"name":"目标检测","slug":"目标检测","permalink":"http://blog.357573.com/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"},{"name":"论文解读","slug":"论文解读","permalink":"http://blog.357573.com/tags/%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB/"},{"name":"算法","slug":"算法","permalink":"http://blog.357573.com/tags/%E7%AE%97%E6%B3%95/"},{"name":"图像处理","slug":"图像处理","permalink":"http://blog.357573.com/tags/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/"},{"name":"YOLO","slug":"YOLO","permalink":"http://blog.357573.com/tags/YOLO/"}]},{"title":"目标检测-Faster RCNN","slug":"目标检测-Faster-RCNN","date":"2020-01-05T13:47:21.000Z","updated":"2020-01-06T00:46:41.541Z","comments":true,"path":"2020/01/05/目标检测-Faster-RCNN/","link":"","permalink":"http://blog.357573.com/2020/01/05/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B-Faster-RCNN/","excerpt":"","text":"哈喽大家好，我是蒙特卡洛家的树。时间到了2016年，fastrcnn问世一年，何凯明老师就立刻提出了更快的算法，fast的名字被占用了，所以干脆就叫faster rcnn了。之前fastrcnn上说他跑一帧只需要0.几秒，其实这里玩了一个文字游戏，因为这里跑一帧只是神经网络部分，并没有涵盖region proposal的时间。而fast rcnn最耗时的部分就是region proposal部分，那一步就已经是秒级别的了，所以要优化速度不得不从这里开刀。而何凯明老师提出的fasterrcnn就正是从这里入手，大大优化region proposal的速度。使得全部过程在GPU上跑可以达到一秒五帧了。 我们知道，之前的RCNN region proposal的方法用的都是基于梯度的selective search，这个方法目前没有现成利用GPU版本的，所以会成为一个瓶颈。论文上还说了一句，把他写成GPU的也不是不可以，只不过这是工程上的优化了，做了这个我们就错事做研究的机会了。哈哈。皮一下很开心。那么不用selective search用什么去找候选框呢？这就是这篇论文所提出来的RPN网络，region proposal network。直接用神经网络去找候选框。因为据观察FAST RCNN本身就可以生成边框，所以干脆直接在网络之前加上一个神经网络，让他去生成边框就好了。 RPN是由一个一个anchor组成的，其实只是换汤不换药，基本上还是传统的金字塔搜索方法，预设了选三种尺度，三种长宽比，一共九种大小anchor在featuremap上滑动，因为featuremap会有差不多2400个点，所以最后会生成2400*9个anchor，当然这个9是可变的，你还可以设置4种尺度，5种长宽比的20种anchor。anchor扫过每个点都预测一下这个是不是一个物体，然后回归出应该的bbox，所以一共会输出2400*9*6个值。这里其实概率只需要一个值就够了，但是因为他是用softmax来输出概率的，而softmax的要求就是和为1，所以只好把是和否分开了。 由于anchor是滑动窗，所以同一个proposal他是平移不变的。而基于kmeans的multibox就不能保证全图不变性。但是这个multibox后来成了SSD的核心步骤了，不过SSD的弊端也就因他而起，当然这是后话。 另外，由于multibox是基于全图的全连接做法，输出800维的全连接，所以参数量其大，而anchor的方法由于是类似卷积操作，每个滑块参数都是一样的，都是512维的，再乘以输出维度6和anchor种类9，一共也才两万八千个参数。 在训练的时候选取IOU大于0.7的框作为正样本，IOU小于0.3的框作为负样本训练。 Loss函数和FAST RCNN一样，只不过class的个数改成了2，就是是与否。 接下来就只用接上fastrcnn的网络就可以了。我们来看一下结果，由于也采用了神经网络做计算原来需要两秒的region proposal现在也做到了毫秒级别，整个流程下来可以达到接近实时了。所以RPN的出现同样也是一个质的飞跃。 好了，FASTER RCNN也介绍到这里了。谢谢大家。","categories":[{"name":"论文解读","slug":"论文解读","permalink":"http://blog.357573.com/categories/%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB/"}],"tags":[{"name":"目标检测","slug":"目标检测","permalink":"http://blog.357573.com/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"},{"name":"论文解读","slug":"论文解读","permalink":"http://blog.357573.com/tags/%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB/"},{"name":"RCNN","slug":"RCNN","permalink":"http://blog.357573.com/tags/RCNN/"},{"name":"算法","slug":"算法","permalink":"http://blog.357573.com/tags/%E7%AE%97%E6%B3%95/"},{"name":"图像处理","slug":"图像处理","permalink":"http://blog.357573.com/tags/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/"}]},{"title":"目标检测-Fast RCNN","slug":"目标检测-Fast-RCNN","date":"2020-01-05T13:39:02.000Z","updated":"2020-01-06T00:46:41.493Z","comments":true,"path":"2020/01/05/目标检测-Fast-RCNN/","link":"","permalink":"http://blog.357573.com/2020/01/05/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B-Fast-RCNN/","excerpt":"","text":"哈喽大家好，我是蒙特卡洛家的树，时间来到2015年，经历了一年的发展，官方版的快速RCNN问世了。这次出的这个fast rcnn主要借鉴了SPP的方法，共享了featuremap，大大提升了RCNN的效率，另外他自己也有一些创新。就是舍弃了之前的SVM和bbox回归两步，直接在神经网络中完成分类和回归。 他的过程分这么几步。首先是CNN，整张图片输入到神经网络中,算出一个大的featuremap。 然后每个proposal按照比例在featuremap上裁剪丢入空间金字塔池化层。 这时候算出来的不再作为线性的特征了，而是把他当做一个正常的池化，输出的是二维的featuremap，接下来经过一个全连接层。 这个全连接层连接了一对兄弟层，一个输出分类，一个输出bbox。 像SPPNET一样，他是先把整张图经过了一次卷积，然后每个proposal丢到SPP层中，输出2000个固定大小的featuremap。 这里举了例子，因为用了空间金字塔池化，训练的时候一张图有64个ROI，那么由于只需要进行一次前传，所以比RCNN快了64倍。假设batch size是128，那么就选两张图，一张抽64个roi就可以了。 这个网络最大的改进在于他在SPP之后又加入了softmax，直接在网络种算出分类和坐标。把之前的CNN，SVM，坐标回归整合到了一步，这样节省了时间，也避免了硬盘交互。 Fast RCNN的输出用了两个兄弟层，一个是K+1维的softmax概率，K就是想要的类别，+1是加上了背景这一类别。另一个层就是bbox的回归偏移量，这个具体的内容和以前分开的那个回归一样，不过这个是放在了神经网络中做的。 这两个层一起组成了LOSS函数，就是概率的loss加上lambda倍的回归loss。概率的loss用的就是对数损失函数，-LOGpu 而bbox的回归损失函数则是这样的，如果真实的和预测值差&lt;1那么就是0.5倍的差的平方，如果&gt;1就说明预测偏差比较大了，那么损失函数就大一点，就是这个差-0.5 训练时候一个batch是128个ROI，从两张图里得来，一张图取64个ROI。其中有25%的roi是iou大于0.5的，这些作为前景，iou在0.1-0.5之间的作为背景。剩下的作为保留。 Fast RCNN另一个最大的创新就是他发明了一种方法，大大减少了两个连续的全连接层之间的参数个数。假设两个全连接层输出分别维u和v，那么中间的参数就会是u*v个。但是如果中间加上一个过渡层，他的输出是t，那么这三层的参数就应该是u*t+v*t了，而如果我们让t很小，则参数总数就会比u*v大大的减小了。而怎么样让t很小却又步丢失重要信息呢？其实这个方法就叫做降维，在PCA里面属于基本操作了，而PCA的核心就是使用了SVD的方法。对u*v那么大的权重矩阵进行奇异值分解，选出奇异值最大的前t个奇异向量作为中间层的权值就可以了。这一步堪称画龙点睛之笔，在过去流行多个全连接层网络的时代有着重要意义。 我们再来看看结果。可以看到，由于删繁就简，FAST RCNN的训练速度只需要一两个小时了，相比之下RCNN和SPP都需要一天多。而测试速度也是高达0.几毫秒了，而SPP颠覆及的加速也才2s。最后看准确率的话其实没什么变动，所以这个网络之所以叫fast rcnn就是因为速度大大的提高了。","categories":[{"name":"论文解读","slug":"论文解读","permalink":"http://blog.357573.com/categories/%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB/"}],"tags":[{"name":"目标检测","slug":"目标检测","permalink":"http://blog.357573.com/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"},{"name":"论文解读","slug":"论文解读","permalink":"http://blog.357573.com/tags/%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB/"},{"name":"RCNN","slug":"RCNN","permalink":"http://blog.357573.com/tags/RCNN/"},{"name":"算法","slug":"算法","permalink":"http://blog.357573.com/tags/%E7%AE%97%E6%B3%95/"},{"name":"图像处理","slug":"图像处理","permalink":"http://blog.357573.com/tags/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/"}]},{"title":"目标检测-SPPNet","slug":"目标检测-SPPNet","date":"2020-01-05T13:15:40.000Z","updated":"2020-01-06T00:46:41.545Z","comments":true,"path":"2020/01/05/目标检测-SPPNet/","link":"","permalink":"http://blog.357573.com/2020/01/05/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B-SPPNet/","excerpt":"","text":"哈喽大家好，我是蒙特卡洛家的树。RCNN横空出世之后，人们开始把目标检测的重心放在了神经网络上。但是RCNN有个非常明显的缺点，就是速度慢，以为有2000个候选框需要计算，所以跑了2000次前向传播。这样跑一张图片就需要五十多秒。何凯明老师提出了一种新的思想，大大的提升了性能，这个方法叫做空间金字塔池化。 这个空间金字塔池化使用了比例池化,可以不关心输入大小,也能够输出固定维度的特征,从而代替全连接层.其实就是规定把图片分成几份，传统的池化是规定多少大小为一份，而spp是直接约定分成几份，至于一份的大小就可变了。 另外还有一个改进是特征共享，原先2000个proposal分别进入神经网络去算出2000个特征，这样效率是非常低下的。经过实验spp证明了只需要把原图走一遍神经网络，得出最后一个featuremap，因为有了空间金字塔池化层，不管输入是什么输出大小都是一样的，所以这时候把最后那层featuremap按照比例裁剪，再分别丢进池化层输出特征 先说比例池化，我们可以看到，图中最后那个featuremap经过了三个池化层，,第一个池化层为4*4,不是指的卷积核大小是4*4,而是将输入的feature图分成4*4的做池化,例如使用最大池,那么每个区域取最大值,这样就能输出16个数,这16个数分别是每个区域的最大值,这就是这个池化层输出的结果.第二个池化层为2*2,那么同样这一层会输出4个数,第三个池化层为1*1,那么会输出一个数,这16+4+1=21个数就是要输出的21个特征.因为他是按照比例去输出的,所以不需要关心图像大小,只要到最后还能被切成这么多块就可以了. 正是因为有了这一特性，我们可以不用关心输入大小了，所以假设上一层卷积输出的featuremap中间随便截取一个小框也能够输入到这个池化层里面。所以我们可以把整张图直接丢进神经网络，算出整张图的featuremap，然后再把候选框那个位置按照比例在featuremap上截取下来，依然能够输入到空间金字塔池化层。我们看图，左边这个车轮在这里，对应的featuremap里的这里，假设我们proposal出来的区域有这个车轮，那么我们直接把他在featuremap上对应的这个区域截取下来，输入到池化层就行了。这样计算量大大的减小了，因为2000个proposal都是从这一个featuremap上抠下来的特征，相当于整个流程只做了一次前向传播。 由于输入大小可变这个特性,spp训练的时候就可以把训练图片按比例缩放,这样可以训练不同尺度的图片,相当于增加了数据量.训练多尺度的时候首先规定几种默认尺度,如224的和180的,先手动设置polling层,训练224的一个回合,然后在更改polling层训练180的，这样来回交替，据验证效果要比一个尺度的更好。","categories":[{"name":"论文解读","slug":"论文解读","permalink":"http://blog.357573.com/categories/%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB/"}],"tags":[{"name":"目标检测","slug":"目标检测","permalink":"http://blog.357573.com/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"},{"name":"论文解读","slug":"论文解读","permalink":"http://blog.357573.com/tags/%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB/"},{"name":"RCNN","slug":"RCNN","permalink":"http://blog.357573.com/tags/RCNN/"},{"name":"算法","slug":"算法","permalink":"http://blog.357573.com/tags/%E7%AE%97%E6%B3%95/"},{"name":"图像处理","slug":"图像处理","permalink":"http://blog.357573.com/tags/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/"},{"name":"SPP","slug":"SPP","permalink":"http://blog.357573.com/tags/SPP/"}]},{"title":"目标检测-RCNN","slug":"目标检测-RCNN","date":"2020-01-02T07:31:39.000Z","updated":"2020-01-02T09:17:07.078Z","comments":true,"path":"2020/01/02/目标检测-RCNN/","link":"","permalink":"http://blog.357573.com/2020/01/02/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B-RCNN/","excerpt":"","text":"rcnn首先会做一个region proposal，也就是任意的找到一些疑似完整物体的区域，这一步是任意的，根据梯度信息找到一些边缘，然后就圈出来了。这一步会找到非常多的区域作为候选框，给接下来的分类提供素材。论文说的是找了两千多个 接着把那些候选框一个一个输入神经网络，算出特征。 接着把算出的特征交给svm去做分类，得到分类与置信度。 最后每个类别在训练出四个向量，保证预测的时候特征*这四个向量可以得到x,y,w,h，这样坐标的缩放和偏移量也就算出来了。 所谓的候选区域推荐其实就是根据梯度信息找到一些边界，然后选出一个个疑似完整的物体。 而RCNN的region proposal用的是selective search的方法，这种方法说来也简单，就是每两个相邻区域计算相似度。 如果相似度大于阈值就合并，接着循环，直到不能再合并为止。而一开始是每个像素点为一个区域所以这个过程就是由点逐步变大的过程。 这一步做完了，图中就会出现很多的候选框，然后每个候选框经过神经网络，算出特征。 他这个网络就用的是alexnet，模型也直接拿的imagenet上训练好了的，这样他就不需要从头训练，只需要对输出类别进行一下finetune就行了。由于alexnet输入是固定的227*227的，所以刚才region proposal出来的每一个候选框都resize成227*227输入到这个网络中，算出4096维的特征。注意啊，这个图上是224的，实际上是错误的，因为和alexnet论文下面自相矛盾了，他下面输出是55，而步长是4，核大小是11，所以输入必须是227才能被整除。因为我们这一步只需要提取特征，而alexnet是为了在imagenet上进行分类，所以最后又加了一层1000维的全连接，是因为imagenet是有1000个分类，所以弄了1000维的全连接加上softmax，我们这一步只需要提取特征，就不需要softmax了，所以我们这一步得到的结果是每个候选框算出一个4096维的特征，假设有n个候选框就会算出n个4096维的特征。 而实际上论文也说了，这个n就是2000左右，我们会找两千个左右的区域。所以最后经过CNN会出现2000个4096维的特征向量。 这个神经网络训练的时候加了一层21维的全连接层以及softmax，因为PASCAL VOC比赛要求的是20种分类，那么加上背景就是21种了。这里我们可以根据自己的需要任意改变这个输出种类。他在训练的时候每个区域判断和groundtruth的IOU，就是交集比上并集，如果IOU&gt;0.5就认为是相应的物体，小于就当做背景。然后学习率设的很小，只有0.001，因为是finetune的，每次迭代选了32个正样本，96个背景作为一个batch去训练。 最后拿着这些特征去训练SVM，SVM训练时候也是有讲究的，他的正样本直接取的是groundtruth了，而负样本采用的是IOU&lt;0.3的proposal，中间的全部扔掉。 为什么要扔掉呢？他后面也写了，因为这个候选情况太多了，我们选择最有代表的样例就行了，不然的话因为允许有偏差，正样例数量可能会扩大30倍 也就是因为训练CNN时候用的正样本是IOU&gt;0.5的，背景是IOU&lt;0.5。而训练SVM的时候正样本就是IOU=1的，负样本是IOU&lt;0.3的，就是这里对位置要求的严格，使得分类的准确率大大提升，所以最终选择使用加一步SVM的分类，而不是直接采用softmax进行分类。 分类结果出来以后就该算出准确位置了，因为这些候选框都是一开始第一步粗略挑选出来的，期初他并不知道类别，所以位置和大小也并不一定准确，所以接下来要找到这个框的精确位置。我们说精确位置，其实包含了框的位置和大小，所以我们要得到的也就是4个数字，分别是deltax，deltay，wscale，hscale，分别表示真实边框相对于预测的这个框左上角顶点偏移量，以及长宽的放大尺度。这四个值其实是能够根据特征学习出来的。因为我们上一层也就是CNN输出的是4096维的特征，这一层要输出的是4个数，假设我们有4个4096维的向量，和那4096维的特征相乘，假设特征是1行，4096列，而这个向量是4096行1列，那么乘出来就是1个数了。把这个数当做deltax，那么那个向量就是deltax的参数了，同样的道理，每个类别求出4个这样的向量，分别就是deltax，deltay，wscale，hscale的参数向量了。这个向量明显是可以学习的出来的，因为不同的特征代表物体不同区域，其实是有规律可循的，所以这个变换的向量也就一定可以学习的出来的。这个学习的过程其实就是边框回归。 回归训练的时候一定要用IOU&gt;0.6的样本训练，太小了实在很难看出是什么，对于这种简单的线性参数干扰很大，所以这一步其实只适合用来微调，训练的时候一定要控制好IOU。 总结 有了这几步，RCNN的全部流程就出来了，先用selective search找到2000个region proposal，再用finetune过的alexnet算出2000个4096维的特征，分别计算这2000个对应的类别和置信度，然后用bbox regression去回归他准确的位置。","categories":[{"name":"论文解读","slug":"论文解读","permalink":"http://blog.357573.com/categories/%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB/"}],"tags":[{"name":"目标检测","slug":"目标检测","permalink":"http://blog.357573.com/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"},{"name":"论文解读","slug":"论文解读","permalink":"http://blog.357573.com/tags/%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB/"},{"name":"RCNN","slug":"RCNN","permalink":"http://blog.357573.com/tags/RCNN/"},{"name":"算法","slug":"算法","permalink":"http://blog.357573.com/tags/%E7%AE%97%E6%B3%95/"},{"name":"图像处理","slug":"图像处理","permalink":"http://blog.357573.com/tags/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/"}]},{"title":"Hello World","slug":"hello-world","date":"2020-01-02T00:23:48.697Z","updated":"2020-01-02T00:23:48.697Z","comments":true,"path":"2020/01/02/hello-world/","link":"","permalink":"http://blog.357573.com/2020/01/02/hello-world/","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","categories":[],"tags":[]},{"title":"hexo 博客搭建","slug":"hexo-博客搭建","date":"2020-01-01T02:42:40.000Z","updated":"2020-01-06T00:46:41.629Z","comments":true,"path":"2020/01/01/hexo-博客搭建/","link":"","permalink":"http://blog.357573.com/2020/01/01/hexo-%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA/","excerpt":"","text":"备份因为hexo提交到git上只有生成的html文件，没有源码文件，导致一旦换了电脑就直接gg了，所以我找了一个插件GitHub 使用方法npm install hexo-git-backup --save 在_config.yml中添加一项backup:&emsp;&emsp;type: git&emsp;&emsp;repository:&emsp;&emsp;&emsp;&emsp;github: git@github.com:xxx/xxx.git,branchName之后每次用hexo b就可以备份上传了注意一点，这里的分支一定要和master分支区分开，master就专门用来存放网站的，源码放在另一个分支上，可以新建一个source分支，然后把source分支设置成主分支，这样的话下次你git clone就可以直接clone代码了。 远程配置这玩意自动关联的git，所以需要设置一下，也是在_config.yml中，它本来就有deploy,所以补充一下就行了deploy:&emsp;&emsp;type: git&emsp;&emsp;repo: git@github.com:xxx/xxx.git&emsp;&emsp;branch: master之后使用hexo d就是上传到git了，不过也需要安装一个插件npm install hexo-deployer-git --save 每次上传之后github的域名都会失效，所以在站点应该指定域名，具体为在public目录里新建CNAME文件，中间输入域名即可。例如： 1blog.357573.com 基础操作以上配置好了之后实际上就可以用了，如果要新建文章则使用hexo new 文章名,注意这里文章名如果有空格的话记得用斜杠转译，就像我这篇文章的标题一样，是用的 hexo new hexo\\ 博客搭建新建好的文件在source这个目录里，其实你输入了new之后命令行打印出来了文件路径，就是那个md文件，markdown格式的，去了修改就行了。写好之后渲染成html的，使用hexo g也就是generator渲染，如果要本地测试的话就是hexo s，开启一个server，提交到git就用hexo d deploy，上传源码就使用hexo b backup hexo环境搭建我把这个放到最后，是因为这个的资料太多了，前面几个都是我个人总结的，比较难查到的，这个是想着做事做到尽善尽美才加上的。首先下载nodejs，因为hexo是用node写的，地址在https://nodejs.org/en/解压之后把这些放到/usr/local下面就行了然后node的安装源可以换成国内的加速npm config set registry https://registry.npm.taobao.org当然，这个很显然是可有可无的然后是安装hexo了npm i hexo-cli -g这样就算部署完毕了 更换主题这个我觉得挺重要，默认的那个landscape的主题确实太土了，所以可以去主题仓库找找看https://hexo.io/themes/ 找到满意的之后进入theme目录，然后把主题clone下来比方说我这个主题git clone https://github.com/fi3ework/hexo-theme-archer.git下好之后在_config.yml中配置一下theme，就是目录名就可以了。 统计访问量我这个主题是自带统计访问量的，但是需要安装一个插件，所以需要执行npm i --save hexo-wordcount然后修改在 archer 的配置中的reading_info: true不过busuanzi这个js偷偷的把pv、uv的内容隐藏了，就是加上了“display: none;”属性，我在网上看见有人说需要人工去打开，就是在这个主题的css里，我这个主题是在_intro.scss文件里面，加上了 1234567891011121314#busuanzi_container_site_pv &#123; &#x2F;* 网站页脚访问pv *&#x2F; display: inline !important;&#125;#busuanzi_container_site_uv &#123; &#x2F;* 网站页脚访问uv *&#x2F; display: inline !important;&#125;#busuanzi_container_page_pv &#123; &#x2F;* 文章访问pv *&#x2F; display: inline !important;&#125; 但是呢。我这个加了还是有，没办法，我只好去 “base-footer.ejs”文件里面把pv的id去掉，变成 12345678&lt;% if(theme.busuanzi_pv_or_uv &#x3D;&#x3D;&#x3D; &#39;pv&#39;) &#123; %&gt; &lt;span&gt;&lt;%- sloganPieces[0] %&gt;&lt;span id&#x3D;&quot;busuanzi_value_site_pv&quot;&gt;&lt;&#x2F;span&gt;&lt;%- sloganPieces[1] %&gt;&lt;&#x2F;span&gt; &lt;% &#125; else if (theme.busuanzi_pv_or_uv &#x3D;&#x3D;&#x3D; &#39;uv&#39;) &#123; %&gt; &lt;span&gt;&lt;%- sloganPieces[0] %&gt;&lt;span id&#x3D;&quot;busuanzi_value_site_uv&quot;&gt;&lt;&#x2F;span&gt;&lt;%- sloganPieces[1] %&gt;&lt;&#x2F;span&gt; &lt;% &#125; %&gt; &lt;&#x2F;div&gt; &lt;% &#125; %&gt; &lt;script async src&#x3D;&quot;&#x2F;&#x2F;busuanzi.ibruce.info&#x2F;busuanzi&#x2F;2.3&#x2F;busuanzi.pure.mini.js&quot;&gt;&lt;&#x2F;script&gt; 评论功能我的主题支持valine评论，说是无后端的评论，但是我感觉是存在它们那里。因为需要注册账号。可以参照这个https://valine.js.org/quickstart.html然后在主题的配置文件中把key添加进去就可以了","categories":[],"tags":[]}]}