---
title: 目标检测-YOLO
date: 2020-01-05 21:54:38
tags: [目标检测,论文解读,YOLO,算法,图像处理]
categories: 
- 论文解读
---
![在这里插入图片描述](1.png)
　　RCNN系列出来后对整个目标检测起到了颠覆性的作用，但是由于过于复杂的流程，使得代码实现以及运行速度都很慢。此时，一位二次元大叔站了出来，提出了我们要舍弃这种繁琐， yolo！yolo问世的时代是在fasterrcnn之后，所以论文主要是和rcnn系列做对比，其实这也应证了rcnn系列在目标检测领域的的地位，出的新算法都来和他们对比。那么yolo和fastrcnn比有什么优势呢。

　　第一个是快速，yolo没有region proposal那一步，在Titanx上可以达到45帧。

　　第二个是全局化,yolo是整张图输入的，对于全局信息，尤其是背景的判断要比fastrcnn好。因为fastrcnn都是拆开一块一块的输入，失去了重要的全局信息。


　　第三个是泛化,YOLO可以学到更泛化的特征,即在真实照片上训练,但是在画上面识别,他的这个泛化效果是目前世界最好的.
![在这里插入图片描述](2.png)
　　当然,文章也说了YOLO存在的一些缺点,第一个就是精度不是最好的,毕竟YOLO是在速度上的改革,可以说是一场速度革命了,抛弃了RCNN系列繁琐的流程,找到了一个速度与精度的平衡点. 其次对于定位的精度也不太好,框很有可能预测的比较偏 最后还有对小件很不友好,这是这个算法的原理导致的,因为他最后做了一步类似于压缩的步骤.
![在这里插入图片描述](3.png)
　　我们来看一下YOLO的具体流程,YOLO文章说他是把图像切成S\*S个小方格来检测的,其实这样说容易让人产生误会,误认为它还是和RCNN那样先选出一些region proposal,只不过这次是直接切割出来的了,然后再分别输入神经网络预测.其实YOLO并不是这样的,实际上YOLO就是一个很普通的卷积神经网络,最后靠着全连接层输出了30个S\*S长度的特征. 我们来观察一下这个结构图,首先输入的是448\*448大小的图,经过了一开始的步长为2的卷积,变成了224\*224的featuremap,然后再经过步长为2的池化,就变成了112\*112的featuremap了,也就是图上的第一个输出,接着经过了一系列步长为2的操作,最终缩放成了7\*7的featuremap了,此时用全连接层去处理它,输出了一个4096的全连接特征,然后再加上一层全连接,输出d就是7\*7\*30的特征,这个图上画的是个7\*7\*30的featuremap,实际上这个也是在误导人,根据我仔细观察各个版本的代码实现,最后那一层就是个全连接层,输出的其实是个向量,那个向量的长度是7\*7\*30=1470,为了形象的表达所以在图上又给他画成了三维的形状.所以大家不要被误导了,最后一层是通过全连接层输出的1470维的特征.那你要问了,我们这是个检测网络呀,要输出特征干嘛呢,最后是不是像fastrcnn那样后面再跟一个回归层呢,用这个特征q乘以坐标的参数得到具体位置呢?其实如果这样那这个就成了fastrcnn了,而yolo是完全摒弃了rcnn的繁琐步骤的,所以才叫you only look once,我们得到的这1470维的特征已经包含了所有物体的位置以及概率了.这是怎么实现的呢,我们就要来看看他为什么非要误导我们把最后那个特征画成7\*7\*30这种形状.因为抽象来讲,yolo是把图像切分成S\*S块,然后来分别判断每一块是不是包含了要检测的对象,他的概率是多少,具体坐标偏移量是多少,而这个S预设的就是7.
![在这里插入图片描述](4.png)
　　所以到最后的那个特征,我们可以想象成是把图像切分成了7\*7个小格子得来的,其实看之前那个层,在全连接之前,确实是缩放到了7\*7了,而且步长一直都是2,所以其实这7\*7个点确实是从这些格子里提取出来的特征.
　　再说为什么是30,我们希望每个格子给出一个两个坐标和置信度,就是说以这个格子为中心的物体,给出两组猜测结果,坐标自然是xywh四个了,置信度就是这个框是个物体的概率,所以坐标加上置信度一共是5位了,我们希望每个格子预测出两个bbox出来,那么就是10位了.然后还要输出每个格子代表每种物体的概率是多少,因为Pascal的数据是20类,那么就再加上20个位置放概率,所以总共就是30位了.
![在这里插入图片描述](5.png)
　　这里容易搞混淆,明明不是每个bbox都输出了一个置信度吗,怎么还又输出一个概率呢?而且为什么置信度的数量是\*2了的,概率为什么不用乘呢? 其实我们说每一个格子都会预测出2个bbox,选出两个来就是为了增加一种可能,在现在最新版的YOLO代码里他就改成了3个,所以最后输出的是7\*7\*35长度的向量了.当然这个数字是可以任意变动的,只要能找到效率的平衡点就好.因为YOLO已经把图缩放成了7\*7了,所以现在这个精度很低了,所以我们多提供一个候选框,其实就是在减小风险,避免本身这个物体就落在你这一格里,结果你却不负责任的预测错了.你这一格管着这么大的面积,万一我这个东西就只存在你这一格里面,你预测错了岂不就是没有补救的机会了?所以我们就让一格提供2个bbox,然后呢每个bbox的位置是不一样的,但是中心肯定是在这个格子里的,具体形状不一样,就像这张图上一样,同样中心在这个格子里,但是有的是长条有的是短的,有的大有的小一样.我们每个格子选出2个候选框,就有2组坐标,2个置信度,这个置信度只表示我这个候选框里有要找的物体的概率,不代表是哪种物体,我们可以简单理解成最后从这2个候选框中选择置信度最高的那个作为最终这一格子的候选框,而之后那20个物体的概率,指的就是我们这个格子代表的是每种物体的概率了.那你要问凭什么你规定他是什么他就是什么呢?这里就是神经网络神奇的地方了,想要什么就有什么,完全是通过训练来规定出来的,我们一直规定前面20位表示每种物体的概率,训练的时候也一直用概率来评判他,自然他就会乖乖的变成概率了,后面的是坐标,训练时候也一直用坐标来约束他,他也慢慢的就x了,认识到这里是坐标了.
![在这里插入图片描述](6.png)
　　我们来看他的损失函数,损失函数是把我们刚才介绍的那几项分别求出损失值然后加起来的,使用了最简单的平方差损失函数,预测坐标-实际坐标平方求和,预测概率-实际概率平方求和.其中我们可以看到这里有2个lambda,这是因为大部分的格子都是没有物体的,而没有物体真实的概率就是0,这样很容易就把网络带偏了,所以这里加上一个lambda来减小它的影响,这里的noobj就是没有物体的意思,没有物体时候的置信度误差\*了一个lambda,这个lambda设置的是0.5,而有物体时候的lambda设置的是5,这样就可以平衡两者的影响了.然后x和y代表的是中心点坐标,这个中心店肯定是存在单元格内的,她的值是在单元格内的比例,所以是在0-1之间的,比方说这个点正好在这一格的正中间的话那么x和y就都是0.5,w和h肯定不会局限在单元格内,因为这个格子只代表物体的中心,物体整个d大小是可以大到整张图的,所以w和h就用的是和整张图的比例,这样也把他限制在了0-1之间了,由于大的目标这个误差就很大,所以再开个根号来减小物体过大的影响.
![在这里插入图片描述](7.png)
　　由于每个格子只会输出一个物体,那么如果训练的时候有两个物体落在这个格子里该怎么办呢?YOLO的做法是根据IOU找到IOU最大的一个物体训练,也就是先预测一波,然后看哪个bbox和实际标注的交并比最大就选择那个. yolo在训练的时候其实是先用imagenet做了预训练的,也就是首先训练一个分类器出来,使得网络具有提取特征的能力了,然后再去按照yolo的规则训练检测器,他的这个检测器实际上就是一种回归的方法,因为全程都是用的cnn算出来的值,激活函数用的是prelu,首先relu是最快的激活函数了,因为他几乎就没有激活什么,只是去掉了负值,而prelu让负的也有了意义,yolo设置的p是0.1,也就是说负值的话就是输出0.1x,正值就直接输出x.训练的时候学习率最初设为10-3,然后慢慢调到10-2,为什么是从低向高呢,因为这只是开始阶段,如果一开始调的是10-2他会出现梯度乱跳的问题,毕竟是从预训练模型过来迁移学习的嘛,紧接着到了10-2之后跑75次,然后再降到10-3,跑30此之后降到10-4再跑30次结束.
![在这里插入图片描述](8.png)
　　再来看结果,YOLO的准确率差不多是百分之六十多,fasterrcnn是70,但是fasterrcnn只能跑到7帧,而yolo却可以跑到实时的级别,速度提高了五倍有余,并且YOLO还有一个快速版的,就是把网络阉割了一些层,效果差了一点,但是速度能达到150帧了,已经数据超级快速的网络了. YOLOv1毕竟也是一个早期的作品了,他的革命性在于颠覆了rcnn系列繁琐的步骤,使得整个检测用一个神经网络做回归就得出来了,为后来的ssd等网络的兴起提供了强大的自信心保证.
