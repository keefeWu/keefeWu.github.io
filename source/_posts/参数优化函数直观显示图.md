---
title: 参数优化函数直观显示图
date: 2020-02-08 15:11:31
tags:
---
转载自：<https://imgur.com/a/Hqolp>

![Long Valley](https://i.imgur.com/2dKCQHh.gif?1)

## Long Valley

Algos without scaling based on gradient information really struggle to break
symmetry here - SGD gets no where and Nesterov Accelerated Gradient / Momentum
exhibits oscillations until they build up velocity in the optimization
direction.Algos that scale step size based on the gradient quickly break
symmetry and begin descent.

![Beale&#039;s function](https://i.imgur.com/pD0hWu5.gif?1)

## Beale's function

Due to the large initial gradient, velocity based techniques shoot off and
bounce around - adagrad almost goes unstable for the same reason.Algos that
scale gradients/step sizes like adadelta and RMSProp proceed more like
accelerated SGD and handle large gradients with more stability.

![Saddle Point](https://i.imgur.com/NKsFHJb.gif?1)

## Saddle Point

Behavior around a saddle point.NAG/Momentum again like to explore around,
almost taking a different path. Adadelta/Adagrad/RMSProp proceed like
accelerated SGD.

  

