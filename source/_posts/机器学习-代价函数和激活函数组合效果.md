---
title: 机器学习-代价函数和激活函数组合效果
date: 2020-02-05 11:12:36
tags: [机器学习]
categories: 
- 机器学习
---
#均方差代价函数 + sigmoid
$$C=\frac {(y−a)^2} 2$$
其中，z=wx+b,
而a是经历了激活函数的z，a=σ(z)。
那么这个的梯度，对w和b分别求导，得到：
$$\frac{∂C} {∂w} =(a−y)σ′(z)x=aσ′(z)$$
$$\frac{∂C} {∂b} =(a−y)σ′(z)=aσ′(z)$$
也就是说梯度的大小和激活函数的倒数有着直接的正相关关系。假设我们选择sigmoid函数
![这里写图片描述](1.jpg)
 
可以很明显的看出，在z = 0的时候导数最大，越往两边越小。也就代表神经网络输出结果离真实值越远学习速率越慢。这种差距越大速率越慢的结果对我们训练很不利，所以我们需要寻求一种新的方法来解决。

# 交叉熵代价函数+sigmoid
我们提出交叉熵的方法，来配合sigmoid可以完美的解决这个问题。
首先，交叉熵的公式是
$$ C = -\frac 1 n \sum_{x} [ylna +(1-y)ln(1-a)]$$

求导可以得到
$$ C = -\frac 1 n \sum_{x} \frac {σ′(z)x_j} {σ(z)(1−σ(z))}(σ(z)−y)$$

由于sigmoid函数是σ(z)=1/(1+e−z)，所以实际上可以推导出σ′(z)=σ(z)(1−σ(z)) （别问我这是怎么发现的，神奇的数学家发现的，我验证过了的），所以可以消除很多项了，也就是
$$ \frac {∂C} {∂w_j}=\frac 1 n \sum_ {x} x_j(σ(z)−y) $$
这样一来速率就只和(σ(z)−y)成正相关了，所以差距越大，速度也就可以越快了。

参考资料：
[神经网络与深度学习--交叉熵代价函数](https://hit-scir.gitbooks.io/neural-networks-and-deep-learning-zh_cn/content/chap3/c3s1.html)
