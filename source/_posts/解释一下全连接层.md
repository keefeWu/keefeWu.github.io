---
title: 解释一下全连接层
date: 2020-02-08 15:20:09
tags:
---
在卷积神经网络的最后，往往会出现一两层全连接层，全连接一般会把卷积输出的二维特征图转化成一维的一个向量，这是怎么来的呢？目的何在呢？

举个例子：

![](0.png)  

最后的两列小圆球就是两个全连接层，在最后一层卷积结束后，进行了最后一次池化，输出了20个12*12的图像，然后通过了一个全连接层变成了1*100的向量。

这是怎么做到的呢，其实就是有20*100个12*12的卷积核卷积出来的，对于输入的每一张图，用了一个和图像一样大小的核卷积，这样整幅图就变成了一个数了，如果厚度是20就是那20个核卷积完了之后相加求和。这样就能把一张图高度浓缩成一个数了。

全连接的目的是什么呢？因为传统的网络我们的输出都是分类，也就是几个类别的概率甚至就是一个数--
类别号，那么全连接层就是高度提纯的特征了，方便交给最后的分类器或者回归。

但是全连接的参数实在是太多了，你想这张图里就有20*12*12*100个参数，前面随便一层卷积，假设卷积核是7*7的，厚度是64，那也才7*7*64，所以现在的趋势是尽量避免全连接，目前主流的一个方法是全局平均值。

也就是最后那一层的feature map（最后一层卷积的输出结果），直接求平均值。有多少种分类就训练多少层，这十个数字就是对应的概率或者叫置信度。

